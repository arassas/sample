{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c06cef",
   "metadata": {},
   "source": [
    "# Step01: Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0750e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions form WF MDD (split datasets, VIF, WOE, mono bin)\n",
    "\n",
    "def mono_bin(temp_df, feature, target, n=10):\n",
    "    # rho for spearman correlation\n",
    "    custom_rho = 1\n",
    "    r = 0\n",
    "    \n",
    "    while np.abs(r) < custom_rho and n > 1:\n",
    "        try:\n",
    "            #quantile discretizer custs data into equal number of bins\n",
    "            qds = QuantileDiscretizer(numBuckets = n, inputCol = feature, outputCol = 'buckets',\n",
    "                                     relativeError = 0.01, handleInvalid = 'error')\n",
    "            temp_df = qds.setHandleInvalid('keep').fit(temp_df).transform(temp_df)  #keep NAN\n",
    "            \n",
    "            #create corr_df is Python implemented\n",
    "            corr_df = temp_df.groupby('buckets').agg({feature: 'avg', target: 'avg'}).toPandas()\n",
    "            corr_df.columns = ['buckets', feature, target]\n",
    "            r,p = stats.spearmanr(corr_df[feature], corr_df[target])\n",
    "            n = n-1\n",
    "            \n",
    "        except Exception as e:\n",
    "            n = n-1\n",
    "            \n",
    "        return temp_df\n",
    "    \n",
    "\n",
    "    \n",
    "# transform in WOE\n",
    "def fit_woe_on_training(path, df, target, label):\n",
    "    lst_tbl, lst_iv = list(),list()\n",
    "    max_bin = 0\n",
    "    \n",
    "    count = -1\n",
    "    for feature in final_vars:\n",
    "        print(\"feature is:\", feature)\n",
    "        #execute if feature is not a target col\n",
    "        if feature != target:\n",
    "            count = count + 1\n",
    "            temp_df = df.select([feature,target])   #spark\n",
    "            \n",
    "            #perform monotonic binning\n",
    "            if feature in num_vars:\n",
    "                temp_df = mono_bin(temp_df,feature,target,n=max_bin)\n",
    "                #numeric values\n",
    "                grouped = temp_df.groupby('buckets')\n",
    "            else:\n",
    "                #categorical\n",
    "                grouped = temp_df.groupby(feature)\n",
    "                \n",
    "            #count and event value for each group\n",
    "            count_df = grouped.agg(F.count(target).alias('count')).toPandas()\n",
    "            event_df = grouped.agg(F.sum(target).alias('event')).toPandas()\n",
    "            \n",
    "            #store min/max for variables\n",
    "            if feature in num_vars:\n",
    "                min_value = grouped.agg(F.min(feature).alias('min')).toPandas()['min']\n",
    "                max_value = grouped.agg(F.max(feature).alias('max')).toPandas()['max']\n",
    "            else:\n",
    "                min_value = count_df[feature]\n",
    "                max_value = count_df[feature]\n",
    "                \n",
    "                \n",
    "            #calculate WOE and IV\n",
    "            temp_woe_df = calculate_woe(count_df, event_df, min_value, max_value, feature)\n",
    "            \n",
    "            #sort by min value and keep increasing order\n",
    "            temp_woe_df.sort_values(by = 'min_value', inplace=True)\n",
    "            temp_woe_df.reset_index(inplace=True)\n",
    "            temp_woe_df.drop(['index'], axis=1, inplace=True)\n",
    "            temp_woe_df.reset_index(inplace=True)\n",
    "            temp_woe_df.rename(columns={'index':'bin'},inplace=True)\n",
    "                \n",
    "            #mapping tavble between bin number and WOE\n",
    "            temp_woe_df['bin_adjust'] = np.where(temp_woe_df['min_value'].isna(), -999, temp_woe_df['bin'])\n",
    "            \n",
    "            #separate IV dataset\n",
    "            iv = pd.DataFrame({'IV': temp_woe_df.groupby('varname').tot_iv.max()})\n",
    "            iv['predictive_ind'] = np.where(iv['IV']>= 0.02, 1, 0)\n",
    "            iv = iv.reset_index()\n",
    "            \n",
    "            # ----------------------------------\n",
    "            # save table for each predictor\n",
    "            # ----------------------------------\n",
    "            woe_iv_dict = temp_woe_df.to_dict()\n",
    "            lst_tbl.append(woe_iv_dict)\n",
    "            \n",
    "            iv_dict = iv.to_dict()\n",
    "            lst_iv.append(iv_dict)\n",
    "            \n",
    "        # ---------------------------------\n",
    "        # save table for each predictor\n",
    "        # ---------------------------------\n",
    "        filename = (path + label + '_woe_tbl_FIT')\n",
    "        pickle.dump(lst_tbl, open(filename,'wb'))\n",
    "        print(\"WOE table saved\")\n",
    "        \n",
    "        filename = (path + label + '_iv_tbl_FIT')\n",
    "        pickle.dump(lst_iv, open(filename,'wb'))\n",
    "        print(\"IV table saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7317c6",
   "metadata": {},
   "source": [
    "## Read Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec922b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phys = spark.sql(\"\"\" select * from table \"\"\")\n",
    "phys = phys.filter( (F.col('week_n') >= 18) )\n",
    "phys.createOrReplaceTempView('phys')\n",
    "\n",
    "print(\"number of obs:\", phys.count())\n",
    "print(\"number of cols:\", len(phys.columns))\n",
    "\n",
    "stats = phys.groupBy(['week_n']) \\ \n",
    "            .agg(F.sum(F.col('target')).alias('tot pos'),\n",
    "                F.count('*').alias('tot rows'),\n",
    "                (F.sum(F.col('target'))/F.count('*')).alias('target rate')).orderBy('week_n')\n",
    "                \n",
    "df = stats.toPandas()\n",
    "\n",
    "#fill missing\n",
    "phys = phys.na.fill(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e7c15",
   "metadata": {},
   "source": [
    "## Split into segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def29a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospital\n",
    "df_hosp_manual = phys.filter( (F.col('hospital_ind') == 1) & (F.col('auto_adj') == 0) )\n",
    "print(\"Hospital manual\")\n",
    "\n",
    "# save parquet\n",
    "df_hosp.write.partitionBy('week_n').mode('overwrite').parquet(path + 'segment1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078862e8",
   "metadata": {},
   "source": [
    "## Split into train/out-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3840ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_lab = ['seg1','seg2','seg3','seg4']\n",
    "\n",
    "for s in seg_lab:\n",
    "    print(s)\n",
    "    seg = spark.read.parquet(path + s + '.parquet')\n",
    "    train = seg.filter( (F.col('week_n') >= 18) & (F.col('week_n') <= 27) )\n",
    "    val = seg.filter( (F.col('week_n') >= 28) & (F.col('week_n') <= 30) )\n",
    "    \n",
    "    print(\"training\")\n",
    "    print(\"number of obs:\", train.count())\n",
    "    print(\"number of cols:\", len(train.columns))\n",
    "    print()\n",
    "    \n",
    "    # save parquet\n",
    "    train.write.partitionBy('week_n').mode('overwrite').parquet(path + s + 'train_cohort.parquet')\n",
    "    print(\"training has been saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a46f3f",
   "metadata": {},
   "source": [
    "## Split into in-time train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39add775",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_lab = ['seg1','seg2','seg3','seg4']\n",
    "\n",
    "for s in seg_lab:\n",
    "    print(s)\n",
    "    \n",
    "    #read saved training\n",
    "    seg = spark.read.parquet(path + s + 'train_cohort.parquet')\n",
    "    \n",
    "    split_datasets(sdf_input = seg,\n",
    "                  path = path,\n",
    "                  train_size = 0.80,\n",
    "                  test_size = 0.20,\n",
    "                  s = s)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36582f",
   "metadata": {},
   "source": [
    "## Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_lab = ['seg1','seg2','seg3','seg4']\n",
    "\n",
    "for s in seg_lab:\n",
    "    print(\"***************************\")\n",
    "    print(s)\n",
    "    # --------------------------\n",
    "    # training\n",
    "    # --------------------------\n",
    "    trn_sdf = spark.read.parquet(path + s + 'in_smpl_trainset.parquet')\n",
    "    print(\"number of obs:\", trn_sdf.count())\n",
    "    print(\"number of cols:\", len(trn_sdf.columns))\n",
    "    stats = phys.groupBy(['week_n']) \\ \n",
    "            .agg(F.sum(F.col('target')).alias('tot pos'),\n",
    "                F.count('*').alias('tot rows'),\n",
    "                (F.sum(F.col('target'))/F.count('*')).alias('target rate')).orderBy('week_n').toPandas()\n",
    "    print(stats)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
