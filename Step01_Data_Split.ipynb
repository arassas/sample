{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c06cef",
   "metadata": {},
   "source": [
    "# Step01: Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0750e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions form WF MDD (split datasets, VIF, WOE, mono bin)\n",
    "\n",
    "def mono_bin(temp_df, feature, target, n=10):\n",
    "    # rho for spearman correlation\n",
    "    custom_rho = 1\n",
    "    r = 0\n",
    "    \n",
    "    while np.abs(r) < custom_rho and n > 1:\n",
    "        try:\n",
    "            #quantile discretizer custs data into equal number of bins\n",
    "            qds = QuantileDiscretizer(numBuckets = n, inputCol = feature, outputCol = 'buckets',\n",
    "                                     relativeError = 0.01, handleInvalid = 'error')\n",
    "            temp_df = qds.setHandleInvalid('keep').fit(temp_df).transform(temp_df)  #keep NAN\n",
    "            \n",
    "            #create corr_df is Python implemented\n",
    "            corr_df = temp_df.groupby('buckets').agg({feature: 'avg', target: 'avg'}).toPandas()\n",
    "            corr_df.columns = ['buckets', feature, target]\n",
    "            r,p = stats.spearmanr(corr_df[feature], corr_df[target])\n",
    "            n = n-1\n",
    "            \n",
    "        except Exception as e:\n",
    "            n = n-1\n",
    "            \n",
    "        return temp_df\n",
    "    \n",
    "\n",
    "    \n",
    "# transform in WOE\n",
    "def fit_woe_on_training(path, df, target, label):\n",
    "    lst_tbl, lst_iv = list(),list()\n",
    "    max_bin = 10\n",
    "    \n",
    "    count = -1\n",
    "    for feature in final_vars:\n",
    "        print(\"feature is:\", feature)\n",
    "        #execute if feature is not a target col\n",
    "        if feature != target:\n",
    "            count = count + 1\n",
    "            temp_df = df.select([feature,target])   #spark\n",
    "            \n",
    "            #perform monotonic binning\n",
    "            if feature in num_vars:\n",
    "                temp_df = mono_bin(temp_df,feature,target,n=max_bin)\n",
    "                #numeric values\n",
    "                grouped = temp_df.groupby('buckets')\n",
    "            else:\n",
    "                #categorical\n",
    "                grouped = temp_df.groupby(feature)\n",
    "                \n",
    "            #count and event value for each group\n",
    "            count_df = grouped.agg(F.count(target).alias('count')).toPandas()\n",
    "            event_df = grouped.agg(F.sum(target).alias('event')).toPandas()\n",
    "            \n",
    "            #store min/max for variables\n",
    "            if feature in num_vars:\n",
    "                min_value = grouped.agg(F.min(feature).alias('min')).toPandas()['min']\n",
    "                max_value = grouped.agg(F.max(feature).alias('max')).toPandas()['max']\n",
    "            else:\n",
    "                min_value = count_df[feature]\n",
    "                max_value = count_df[feature]\n",
    "                \n",
    "                \n",
    "            #calculate WOE and IV\n",
    "            temp_woe_df = calculate_woe(count_df, event_df, min_value, max_value, feature)\n",
    "            \n",
    "            #sort by min value and keep increasing order\n",
    "            temp_woe_df.sort_values(by = 'min_value', inplace=True)\n",
    "            temp_woe_df.reset_index(inplace=True)\n",
    "            temp_woe_df.drop(['index'], axis=1, inplace=True)\n",
    "            temp_woe_df.reset_index(inplace=True)\n",
    "            temp_woe_df.rename(columns={'index':'bin'},inplace=True)\n",
    "                \n",
    "            #mapping tavble between bin number and WOE\n",
    "            temp_woe_df['bin_adjust'] = np.where(temp_woe_df['min_value'].isna(), -999, temp_woe_df['bin'])\n",
    "            \n",
    "            #separate IV dataset\n",
    "            iv = pd.DataFrame({'IV': temp_woe_df.groupby('varname').tot_iv.max()})\n",
    "            iv['predictive_ind'] = np.where(iv['IV']>= 0.02, 1, 0)\n",
    "            iv = iv.reset_index()\n",
    "            \n",
    "            # ----------------------------------\n",
    "            # save table for each predictor\n",
    "            # ----------------------------------\n",
    "            woe_iv_dict = temp_woe_df.to_dict()\n",
    "            lst_tbl.append(woe_iv_dict)\n",
    "            \n",
    "            iv_dict = iv.to_dict()\n",
    "            lst_iv.append(iv_dict)\n",
    "            \n",
    "        # ---------------------------------\n",
    "        # save table for each predictor\n",
    "        # ---------------------------------\n",
    "        filename = (path + label + '_woe_tbl_FIT')\n",
    "        pickle.dump(lst_tbl, open(filename,'wb'))\n",
    "        print(\"WOE table saved\")\n",
    "        \n",
    "        filename = (path + label + '_iv_tbl_FIT')\n",
    "        pickle.dump(lst_iv, open(filename,'wb'))\n",
    "        print(\"IV table saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7317c6",
   "metadata": {},
   "source": [
    "## Read Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec922b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phys = spark.sql(\"\"\" select * from table \"\"\")\n",
    "phys = phys.filter( (F.col('week_n') >= 18) )\n",
    "phys.createOrReplaceTempView('phys')\n",
    "\n",
    "print(\"number of obs:\", phys.count())\n",
    "print(\"number of cols:\", len(phys.columns))\n",
    "\n",
    "stats = phys.groupBy(['week_n']) \\ \n",
    "            .agg(F.sum(F.col('target')).alias('tot pos'),\n",
    "                F.count('*').alias('tot rows'),\n",
    "                (F.sum(F.col('target'))/F.count('*')).alias('target rate')).orderBy('week_n')\n",
    "                \n",
    "df = stats.toPandas()\n",
    "\n",
    "#fill missing\n",
    "phys = phys.na.fill(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77cf2a9",
   "metadata": {},
   "source": [
    "## Basic feature engineering (using spark window functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d8940",
   "metadata": {},
   "source": [
    "### List of initial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedd82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_lst = ['id']\n",
    "lst = ['svc_cat','proc_cd']\n",
    "target = ['target']\n",
    "\n",
    "full_lst = pk_lst + lst + target\n",
    "print(full_lst)\n",
    "\n",
    "#read data by trimming blanks\n",
    "sdf = sdf.select(*full_lst)\\\n",
    "        .withColumn('svc_cat',trim(F.col('svc_cat')))\n",
    "sdf.createOrReplaceTempView(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b254ee",
   "metadata": {},
   "source": [
    "### target rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = sdf.groupBy(['cohort','target'])\\\n",
    "            .agg(F.countDistinct(F.col('id')).alias('mbrs'))\\\n",
    "            .orderBy('cohort').toPandas()\n",
    "\n",
    "stats2 = stats.pivot(index='cohort', columns = 'target', values = 'mbrs').reset_index().add_prefix('target_')\n",
    "stats2['target_rate'] = stats2['target_1']/(stats2['target_0'] + stats2['target_1'])\n",
    "stats2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936b422",
   "metadata": {},
   "source": [
    "### Step01 A: Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d265a1bf",
   "metadata": {},
   "source": [
    "### Get top 5 categories for each ID ranked by total items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf1e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variable to partition\n",
    "var_to_agg = ['proc_cd']\n",
    "\n",
    "#iterate over categorical variables\n",
    "for k in var_to_agg:\n",
    "    \n",
    "    #define variables final ID-level output: count of svc\n",
    "    window_var = ['cohort','id'] + [k]\n",
    "    metrics_var = ['dt_cnt']\n",
    "    modeling_var = ['cohort','id'] + metrics_var + [k]\n",
    "    \n",
    "    #define window partition\n",
    "    windowPartition = Window.partitionBy(window_var)\n",
    "    \n",
    "    #to rank order by total svc from high to low\n",
    "    windowPartition2 = Window.partitionBy('cohort','id').orderBy(F.col('dt_cnt').desc())\n",
    "    \n",
    "    #apply window partition\n",
    "    sdf2 = sdf.withColumn(\"dt_cnt\",approx_count_distinct(F.col('svc_dt')).over(windowPartition))\n",
    "    #sdf2 = sdf2.withColumn(\"cost\",sum(F.col('cst')).over(windowPartition))\n",
    "    \n",
    "    #generate aggregated stats by ID\n",
    "    agg_sdf = sdf.select(*modeling_var).distinct().orderBy(window_var)\n",
    "    \n",
    "    #rank-order total cost by ID\n",
    "    agg_sdf = agg_sdf.withColumn(\"cnt_rank\",dense_rank().over(windowPartition2))\n",
    "    \n",
    "    #select top 5 elements based on cost\n",
    "    agg_sdf2 = agg_sdf.filter(F.col('cnt_rank') <= 5)\n",
    "    \n",
    "    #transform cost amount into 1/0\n",
    "    if k == 'proc_cd':\n",
    "        agg_sdf3 = agg_sdf2.withColumn(\"proc_cd2\",when(F.col('dt_cnt')>0,1).otherwise(F.lit(0)))\n",
    "        k1 = 'proc_cd2'\n",
    "        \n",
    "        #pivot to become ID-level\n",
    "        pivotb = agg_sdf3.groupBy('cohort','id').pivot(k).agg(max(k1))\n",
    "        \n",
    "    else:\n",
    "        #pivot to become Id-level\n",
    "        pivotb = agg_sdf2.groupBy('cohort','id').pivot(k).agg(max(\"dt_cnt\"))\n",
    "        \n",
    "        #get all cols of dataframe into a list\n",
    "        total_columns = pivotb.columns\n",
    "        total_columns.remove('id')\n",
    "        total_columns.remove('cohort')\n",
    "        print(\"total cols to rename:\", len(total_columns))\n",
    "        \n",
    "        #run loop to rename all cols of dataframe with prefix\n",
    "        for j in range(len(total_columns)):\n",
    "            pivotb = pivotb.withColumnRenamed(total_columns[j], (k + '_') + total_columns[j])\n",
    "        print(\"finished renaming cols\")\n",
    "        \n",
    "    #fillna\n",
    "    pivotb2 = pivotb.na.fill(value=0)\n",
    "    \n",
    "    #write as parquet\n",
    "    print(\"writing to parquet ...........\")\n",
    "    filename = (k + '.parquet')\n",
    "    pivotb2.write.partitionBy('cohort').mode('overwrite').parquet(path + filename)\n",
    "    print(filename + \" saved as parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4c28b9",
   "metadata": {},
   "source": [
    "### Verify parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ec635",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_to_agg = ['proc_cd']\n",
    "\n",
    "#iterate over categorical variables\n",
    "for k in var_to_agg:\n",
    "    filename = (k + '.parquet')\n",
    "    print(\"reading filename:\", filename)\n",
    "    sdf = spark.read.parquet(path + filename).cache()\n",
    "    \n",
    "    print(\"number of cols\",len(sdf.columns))\n",
    "    sdf.groupBy('cohort')\\\n",
    "        .agg(F.countDistinct(F.col('id')).alias('mbrs'),\n",
    "            F.count(F.col('id')).alias('tot rows'))\\\n",
    "        .orderBy('cohort').show(10)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b526f20",
   "metadata": {},
   "source": [
    "### Step01 B: Categorical features\n",
    "### For each svc, get days since last event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b069a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_to_agg = ['svc']\n",
    "\n",
    "#iterate over categorical variables\n",
    "for k in var_to_agg:\n",
    "    \n",
    "    #define variables final ID-level output: count of svc\n",
    "    window_var = ['cohort','id'] + [k]\n",
    "    metrics_var = ['days_since_lst_visit']\n",
    "    modeling_var = ['cohort','id'] + metrics_var + [k]\n",
    "    \n",
    "    #define window partition\n",
    "    windowPartition = Window.partitionBy(window_var).orderBy(F.col('days_since_lst_visit').asc())\n",
    "    \n",
    "    #apply window partition\n",
    "    sdf2 = sdf.filter(F.col(\"days_since_lst_visit\")>0).withColumn(\"days_ago_rk\",dense_rank().over(windowPartition))\n",
    "    \n",
    "    #extract first instance of \"days since last visit\"\n",
    "    agg_sdf2 = sdf2.filter(F.col(\"days_ago_rk\") == 1).select(*modeling_var).distinct()\n",
    "    \n",
    "    #rename ID\n",
    "    agg_sdf2 = agg_sdf2.withColumnRenamed('id','id_n')\n",
    "    \n",
    "    #append to list of ID\n",
    "    final_sdf = sdf.select('cohort','id').distinct()\\\n",
    "                    .join(agg_sdf2, (sdf.id == agg_sdf2.id_n), \"left\").drop(*(['id_n']))\n",
    "    \n",
    "    \n",
    "    #pivot to become ID-level\n",
    "    pivotb = final_sdf.groupBy('cohort','id').pivot(k).agg(max('days_since_lst_visit'))\n",
    "    \n",
    "    #fillna\n",
    "    pivotb = pivotb.na.fill(value=0)  #defaulting to day zero\n",
    "    \n",
    "    #remove extra null col\n",
    "    pivotb2 = pivotb.drop(pivotb.null)\n",
    "        \n",
    "    #get all cols of dataframe into a list\n",
    "    total_columns = pivotb2.columns\n",
    "    total_columns.remove('id')\n",
    "    total_columns.remove('cohort')\n",
    "    print(\"total cols to rename:\", len(total_columns))\n",
    "        \n",
    "    #run loop to rename all cols of dataframe with prefix\n",
    "    for j in range(len(total_columns)):\n",
    "        pivotb2 = pivotb2.withColumnRenamed(total_columns[j], (k + '_lst_visit_') + total_columns[j])\n",
    "    print(\"finished renaming cols\")\n",
    "        \n",
    "    #write as parquet\n",
    "    print(\"writing to parquet ...........\")\n",
    "    filename = (k + '.parquet')\n",
    "    pivotb2.write.partitionBy('cohort').mode('overwrite').parquet(path + filename)\n",
    "    print(filename + \" saved as parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c2e4c",
   "metadata": {},
   "source": [
    "### C. One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_hot_encode = ['proc_cd']\n",
    "all_cols = ['cohort','id'] + cols_to_hot_encode\n",
    "\n",
    "df = profile.select(*all_cols).distinct().toPandas()\n",
    "\n",
    "enc = OneHotEncoder(categories = 'auto', handle_unknown = 'ignore')\n",
    "\n",
    "#passing instance \n",
    "enc_data_array = enc.fit_transform(df[cols_to_hot_encode]).toarray()\n",
    "feat_labels = enc.categories_\n",
    "feat_labels = np.array(feat_labels).ravel()\n",
    "renamed_feats = enc.get_feature_names(cols_to_hot_encode)\n",
    "\n",
    "#create pandas dataframe for encoded features\n",
    "df_sklearn_encoded = pd.DataFrame(enc_data_array,columns = renamed_feats)\n",
    "df_sklearn_encoded = df_sklearn_encoded.astype('int')\n",
    "\n",
    "#drop original categorical cols once one-hot encoded\n",
    "drop_raw_cols = df.drop(columns = cols_to_hot_encode,axis=1)\n",
    "drop_raw_cols.head()\n",
    "\n",
    "#join to major dataframe\n",
    "data_out = pd.concat([drop_raw_cols,df_sklearn_encoded],axis=1)\n",
    "print(\"dimension:\", data_out.shape)\n",
    "print(data_out.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff88c2",
   "metadata": {},
   "source": [
    "### joining to categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68344731",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_to_agg = ['svc']\n",
    "\n",
    "#iterate over categorical variables\n",
    "for k in var_to_agg:\n",
    "    filename = (k + '.parquet')\n",
    "    print(\"reading filename:\", filename)\n",
    "    sdf = spark.read.parquet(path + filename).cache()\n",
    "    sdf = sdf.withColumnRenamed('id','id_n')\n",
    "    \n",
    "    #open individial file and join to existng table\n",
    "    final_sdf = final_sdf.join(sdf, (final_sdf.id == sdf.id_n), \"left\").drop(*(['id_n']))\n",
    "    print(\"table join completed with variable:\", k, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdacec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83e7c15",
   "metadata": {},
   "source": [
    "## Split into segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def29a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospital\n",
    "df_hosp_manual = phys.filter( (F.col('hospital_ind') == 1) & (F.col('auto_adj') == 0) )\n",
    "print(\"Hospital manual\")\n",
    "\n",
    "# save parquet\n",
    "df_hosp.write.partitionBy('week_n').mode('overwrite').parquet(path + 'segment1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078862e8",
   "metadata": {},
   "source": [
    "## Split into train/out-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3840ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_lab = ['seg1','seg2','seg3','seg4']\n",
    "\n",
    "for s in seg_lab:\n",
    "    print(s)\n",
    "    seg = spark.read.parquet(path + s + '.parquet')\n",
    "    train = seg.filter( (F.col('week_n') >= 18) & (F.col('week_n') <= 27) )\n",
    "    val = seg.filter( (F.col('week_n') >= 28) & (F.col('week_n') <= 30) )\n",
    "    \n",
    "    print(\"training\")\n",
    "    print(\"number of obs:\", train.count())\n",
    "    print(\"number of cols:\", len(train.columns))\n",
    "    print()\n",
    "    \n",
    "    # save parquet\n",
    "    train.write.partitionBy('week_n').mode('overwrite').parquet(path + s + 'train_cohort.parquet')\n",
    "    print(\"training has been saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a46f3f",
   "metadata": {},
   "source": [
    "## Split into in-time train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39add775",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_lab = ['seg1','seg2','seg3','seg4']\n",
    "\n",
    "for s in seg_lab:\n",
    "    print(s)\n",
    "    \n",
    "    #read saved training\n",
    "    seg = spark.read.parquet(path + s + 'train_cohort.parquet')\n",
    "    \n",
    "    split_datasets(sdf_input = seg,\n",
    "                  path = path,\n",
    "                  train_size = 0.80,\n",
    "                  test_size = 0.20,\n",
    "                  s = s)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36582f",
   "metadata": {},
   "source": [
    "## Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e50dd22",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (4050191432.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\adeli\\AppData\\Local\\Temp\\ipykernel_5148\\4050191432.py\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    stats = phys.groupBy(['week_n']) \\\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "seg_lab = ['seg1','seg2','seg3','seg4']\n",
    "\n",
    "for s in seg_lab:\n",
    "    print(\"***************************\")\n",
    "    print(s)\n",
    "    # --------------------------\n",
    "    # training\n",
    "    # --------------------------\n",
    "    trn_sdf = spark.read.parquet(path + s + 'in_smpl_trainset.parquet')\n",
    "    print(\"number of obs:\", trn_sdf.count())\n",
    "    print(\"number of cols:\", len(trn_sdf.columns))\n",
    "    stats = phys.groupBy(['week_n']) \\ \n",
    "            .agg(F.sum(F.col('target')).alias('tot pos'),\n",
    "                F.count('*').alias('tot rows'),\n",
    "                 (F.count('*') - F.sum(F.col(\"target\"))).alias('tot neg'),\n",
    "                (F.sum(F.col('target'))/F.count('*')).alias('target rate')).orderBy('week_n').toPandas()\n",
    "    print(stats)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731fe5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
