{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54505020",
   "metadata": {},
   "source": [
    "# Step 03: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc1aec",
   "metadata": {},
   "source": [
    "## read modeling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# training\n",
    "# -------------------------------\n",
    "trn_sdf = spark.read.parquet(path + 'train_feat_sel.parquet')\n",
    "print(\"number of obs:\", trn_sdf.count())\n",
    "print(\"number of cols:\", len(trn_sdf.columns))\n",
    "\n",
    "trn_sdf.groupBy(['week_n']) \\ \n",
    "            .agg(F.sum(F.col('target')).alias('tot pos'),\n",
    "                F.count('*').alias('tot rows'),\n",
    "                (F.sum(F.col('target'))/F.count('*')).alias('target rate')).orderBy('week_n').show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46408c",
   "metadata": {},
   "source": [
    "## perform downsampling calcualtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e0b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsample such that we have 20% response rate\n",
    "print(\"***************** BEFORE DOWNSAMPLING ********************\")\n",
    "down_smpl = trn_sdf.agg(F.sum(F.col('target')).alias('tot pos'),\n",
    "                       (F.count('*').alias('tot rows') - F.sum(F.col('target'))).alias('tot neg'),\n",
    "                        F.count('*').alias('tot rows'),\n",
    "                        (F.sum(F.col('target'))/F.count('*')).alias('target rate')).toPandas()\n",
    "                       \n",
    "down_smpl['new_tot_size'] = down_smpl['tot pos']/0.20   #20% desired response\n",
    "down_smpl['majority_smpl_rate'] = down_smpl['new_tot_size']/down_smpl['tot neg']\n",
    "down_smpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bca945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampledata due to large size\n",
    "trn_sdf_smpl = trn_sdf.sampleBy('target',\n",
    "                               fractions = {1:1.0,                                   #100% minority\n",
    "                                           0: down_smpl['majority_smpl_rate'][0]},   #majority\n",
    "                               seed=5432)\n",
    "\n",
    "trn_sdf_smpl.groupBy('target')\\\n",
    "            .agg(F.count('*').alias('tot rows')).show()\n",
    "\n",
    "\n",
    "#save parquet\n",
    "trn_sdf_smpl.write.partionBy('week_n').mode('overwrite').parquet(file + 'train_feat_sel_downsampled.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32318328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta = [#negative after downsampling/#pos after downsamping]*\n",
    "# [#pos before downsampling/#neg before downsampling]\n",
    "\n",
    "print(\"***************** AFTER DOWNSAMPLING ********************\")\n",
    "beta_calc = trn_sdf.agg(F.sum(F.col('target')).alias('tot pos'),\n",
    "                       (F.count('*').alias('tot rows') - F.sum(F.col('target'))).alias('tot neg'),\n",
    "                        F.count('*').alias('tot rows'),\n",
    "                        (F.sum(F.col('target'))/F.count('*')).alias('target rate')).toPandas()\n",
    "\n",
    "beta_calc['beta'] = (beta_calc['tot neg'] / beta_calc['tot pos']) * \\\n",
    "                        (down_smpl['tot pos'][0] / down_smpl['tot neg'][0])\n",
    "beta_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fb165",
   "metadata": {},
   "source": [
    "## downsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb3d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read downsampling dataset\n",
    "trn_sdf = spark.read.parquet(file + 'train_feat_sel_downsampled.parquet')\n",
    "trn_sdf.agg(F.sum(F.col('target')).alias('tot pos'),\n",
    "            F.count('*').alias('tot rows'),\n",
    "            (F.sum(F.col('target'))/F.count('*')).alias('target rate')).show()\n",
    "trn_sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb59467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of primary key (dates, claim, prov_id)\n",
    "pk_lst = ['claim_nbr']\n",
    "\n",
    "# list of numeric var that must be treated as factor in H2o\n",
    "binary_lst = ['hosp_ind']\n",
    "\n",
    "print(\"*****************************\")\n",
    "print(\"Before removing PK\")\n",
    "print(\"*****************************\")\n",
    "# ---------------------------\n",
    "# numeric\n",
    "# ---------------------------\n",
    "num_cols = []\n",
    "for item in trn_sdf.dtypes:\n",
    "    if item[1].startswith('int') | item[1].startswith('double') | item[1].startswith('bigint'):\n",
    "        num_cols.append(item[0])\n",
    "        \n",
    "print(\"Numeric list:\", num_cols)\n",
    "print(\"there are\", len(num_cols), \"numeric attributes, including target\")\n",
    "print()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# categorical\n",
    "# ---------------------------\n",
    "cat_cols = []\n",
    "for item in trn_sdf.dtypes:\n",
    "    if item[1].startswith('string'):\n",
    "        cat_cols.append(item[0])\n",
    "        \n",
    "print(\"Categorical list:\", cat_cols)\n",
    "print(\"there are\", len(cat_cols), \"categorical attributes, including target\")\n",
    "print() \n",
    "\n",
    "\n",
    "\n",
    "#remove PK list\n",
    "print(\"*****************************\")\n",
    "print(\"After removing PK\")\n",
    "print(\"*****************************\")\n",
    "num_lst_n = []\n",
    "for i in num_cols:\n",
    "    if i not in pk_lst and i not in binary_lst:\n",
    "        num_lst_n.append(i)\n",
    "        \n",
    "print(\"numeric list:\", num_lst_n)\n",
    "print('there are', len(num_lst_n), 'numeric attributes, including target')\n",
    "\n",
    "\n",
    "cat_lst_n = []\n",
    "for i in cat_cols:\n",
    "    if i not in pk_lst:\n",
    "        cat_lst_n.append(i)\n",
    "        \n",
    "# add binary variables into categorical\n",
    "cat_lst_n.extend(binary_lst)\n",
    "print(\"Categorical list:\", cat_lst_n)\n",
    "print(\"there are\", len(cat_lst_n), \"categorical attributes, including target\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"*****************************\")\n",
    "print(\"Final list\")\n",
    "print(\"*****************************\")\n",
    "# merge into one list\n",
    "lst = num_lst_n + cat_lst_n\n",
    "print(\"Final list:\", lst)\n",
    "print(\"there are\", len(lst), \"categorical attributes, including target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff14c4",
   "metadata": {},
   "source": [
    "## Launch H2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e80135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysparkling import *\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "hc = H2OContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ce87d",
   "metadata": {},
   "source": [
    "## Transform into h2o dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'target'\n",
    "\n",
    "# --------------------\n",
    "# in-time training\n",
    "# --------------------\n",
    "print(\"preparing training\")\n",
    "trn_sdf_n = trn_sdf.select(*lst)  #only required cols\n",
    "\n",
    "#prepare h2o dataframe\n",
    "train_h2o = hc.asH2OFrame(trn_sdf_n)\n",
    "\n",
    "#prepare final list of attribres - converting categorical as factor\n",
    "for i in cat_lst_n:\n",
    "    train_h2o[i] = train_h2o[i].asfactor()\n",
    "    \n",
    "trn_cols = train_h2o.columns\n",
    "#remove target variable from X frame\n",
    "trn_cols.remove(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad0e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_h2o.shape)\n",
    "train_h2o.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4f3cc",
   "metadata": {},
   "source": [
    "## feature selection using GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b169ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost hyperparameters\n",
    "gbm_params1 = {'learn_rate': [0.01,0.1],\n",
    "              'max_depth':[3, 4, 5],\n",
    "              'ntrees':[200, 300, 400, 500]}\n",
    "\n",
    "#train and validate\n",
    "gbm_grid1 = H2OGridSearch(model = H2OGradientBoostingEstimator(learn_rate_annealing=0.99, seed=542),\n",
    "                                 grid_id = 'gbm_grid1',\n",
    "                                 hyper_params = gmb_params1)\n",
    "\n",
    "gbm_grd1.train(x = trn_cols,\n",
    "              y = y,\n",
    "              training_frame = train_h2o,\n",
    "              seed = 4313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get grid results, sorted by validation AUC\n",
    "gbm_gridperf1 = gbm_grid1.get_grid(sort_by = 'auc', decreasing = True)\n",
    "print(gbm_gridperf1)\n",
    "\n",
    "#grab top GBM model\n",
    "best_gbm1 = gbm_gridperf1.models[0]\n",
    "\n",
    "#plot feature importance\n",
    "best_gbm1.varimp_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb803f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve variable importance\n",
    "varimp = best_gbm1.varimp(use_pandas=True)\n",
    "varimp.to_csv(fileapth + 'feat_sel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6909ff3f",
   "metadata": {},
   "source": [
    "## only keep top contributing features (>=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feat = pd.read_csv(filepath + 'feat_sel.csv')\n",
    "top_feat['sel'] = np.where(top_feat['percentage']>=0.01,1,0)   #filter on contribution of at least 1%\n",
    "\n",
    "# select top features that contribute at least 1%\n",
    "top_sel = top_feat[top_feat['sel']==1]\n",
    "print(\"number of selected features:\", len(top_sel))\n",
    "print(top_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d42a8",
   "metadata": {},
   "source": [
    "## save datasets with selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e86450",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_keep = list(top_sel['variable'])\n",
    "final_lst = pk_lst + ['target'] + lst_keep\n",
    "print(\"there are\", len(final_lst), \"variables\")\n",
    "print(final_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0effe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lst2 = ['week_n','pos_cd_n']\n",
    "\n",
    "#prepare data\n",
    "trn_sdf_n = trn_sdf.select(*final_lst2)\n",
    "\n",
    "print(\"number of obs:\", trn_sdf_n.count())\n",
    "print(\"number of cols:\", len(trn_sdf_n.columns))\n",
    "\n",
    "#save parquet\n",
    "trn_sdf_n.write.partionBy('week_n').mode('overwrite').parquet(file + 'train_feat_sel_r2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32060a",
   "metadata": {},
   "source": [
    "## ===========================================================\n",
    "## Read modeling dataset after R2 feature selection\n",
    "## ==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f25120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Training (downsampled)\n",
    "# -----------------------------------\n",
    "trn_sdf = spark.read.parquet(filepath + 'train_feat_sel_r2.parquet')\n",
    "print(\"number of obs:\", trn_sdf.count())\n",
    "print(\"number of cols:\", len(trn_sdf.columns))\n",
    "trn_sdf.groupBy(['week_n']) \\ \n",
    "            .agg(F.sum(F.col('target')).alias('tot pos'),\n",
    "                F.count('*').alias('tot rows'),\n",
    "                (F.sum(F.col('target'))/F.count('*')).alias('target rate')).orderBy('week_n').show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list o primarey keys\n",
    "pk_lst = ['claim_nbr']\n",
    "\n",
    "# list of numeric var that must be treated as factor in H2o\n",
    "binary_lst = ['hosp_ind']\n",
    "\n",
    "print(\"*****************************\")\n",
    "print(\"Before removing PK\")\n",
    "print(\"*****************************\")\n",
    "# ---------------------------\n",
    "# numeric\n",
    "# ---------------------------\n",
    "num_cols = []\n",
    "for item in trn_sdf.dtypes:\n",
    "    if item[1].startswith('int') | item[1].startswith('double') | item[1].startswith('bigint'):\n",
    "        num_cols.append(item[0])\n",
    "        \n",
    "print(\"Numeric list:\", num_cols)\n",
    "print(\"there are\", len(num_cols), \"numeric attributes, including target\")\n",
    "print()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# categorical\n",
    "# ---------------------------\n",
    "cat_cols = []\n",
    "for item in trn_sdf.dtypes:\n",
    "    if item[1].startswith('string'):\n",
    "        cat_cols.append(item[0])\n",
    "        \n",
    "print(\"Categorical list:\", cat_cols)\n",
    "print(\"there are\", len(cat_cols), \"categorical attributes, including target\")\n",
    "print() \n",
    "\n",
    "\n",
    "\n",
    "#remove PK list\n",
    "print(\"*****************************\")\n",
    "print(\"After removing PK\")\n",
    "print(\"*****************************\")\n",
    "num_lst_n = []\n",
    "for i in num_cols:\n",
    "    if i not in pk_lst and i not in binary_lst:\n",
    "        num_lst_n.append(i)\n",
    "        \n",
    "print(\"numeric list:\", num_lst_n)\n",
    "print('there are', len(num_lst_n), 'numeric attributes, including target')\n",
    "\n",
    "\n",
    "cat_lst_n = []\n",
    "for i in cat_cols:\n",
    "    if i not in pk_lst:\n",
    "        cat_lst_n.append(i)\n",
    "        \n",
    "# add binary variables into categorical\n",
    "cat_lst_n.extend(binary_lst)\n",
    "print(\"Categorical list:\", cat_lst_n)\n",
    "print(\"there are\", len(cat_lst_n), \"categorical attributes, including target\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"*****************************\")\n",
    "print(\"Final list\")\n",
    "print(\"*****************************\")\n",
    "# merge into one list\n",
    "lst = num_lst_n + cat_lst_n\n",
    "print(\"Final list:\", lst)\n",
    "print(\"there are\", len(lst), \"categorical attributes, including target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35fe899",
   "metadata": {},
   "source": [
    "## Transform into h2o frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153fce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'target'\n",
    "\n",
    "# --------------------\n",
    "# in-time training\n",
    "# --------------------\n",
    "print(\"preparing training\")\n",
    "trn_sdf = trn_sdf.select(*lst)  #only required cols\n",
    "\n",
    "#prepare h2o dataframe\n",
    "train_h2o = hc.asH2OFrame(trn_sdf)\n",
    "\n",
    "#prepare final list of attribres - converting categorical as factor\n",
    "for i in cat_lst_n:\n",
    "    train_h2o[i] = train_h2o[i].asfactor()\n",
    "    \n",
    "trn_cols = train_h2o.columns\n",
    "#remove target variable from X frame\n",
    "trn_cols.remove(y)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# split training into X_calib\n",
    "# -------------------------------\n",
    "#print(\"preparing calibration datasets\")\n",
    "#train_calib, test_calib = train_h2o.split_frame(ratios=[0.9], seed=6431)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# in-time testing\n",
    "# --------------------\n",
    "print(\"preparing testing\")\n",
    "tst_sdf = tst_sdf.select(*lst)\n",
    "tst_h2o = hc.asH2OFrame(tst_sdf)\n",
    "\n",
    "#prepare final list of attribres - converting categorical as factor\n",
    "for i in cat_lst_n:\n",
    "    tst_h2o[i] = tst_h2o[i].asfactor()\n",
    "    \n",
    "tst_cols = tst_h2o.columns\n",
    "#remove target variable from X frame\n",
    "tst_cols.remove(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_h2o.shape)\n",
    "train_h2o.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70701f",
   "metadata": {},
   "source": [
    "## =============================================\n",
    "## Fit GBM \n",
    "## ============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost hyperparameters\n",
    "gbm_params2 = {'learn_rate': [0.01,0.1],\n",
    "              'max_depth':[3, 4, 5],\n",
    "              'ntrees':[200, 300, 400, 500]}\n",
    "\n",
    "#train and validate\n",
    "gbm_grid2 = H2OGridSearch(model = H2OGradientBoostingEstimator(learn_rate_annealing=0.99, nfolds=3, seed=542,\n",
    "                                                              gainslift_bins=20),\n",
    "                                                              #calibrate_model = True,\n",
    "                                                              #calibration_frame = test_calib),\n",
    "                                 grid_id = 'gbm_grid2',\n",
    "                                 hyper_params = gmb_params2)\n",
    "\n",
    "gbm_grd2.train(x = trn_cols,\n",
    "              y = y,\n",
    "              training_frame = train_h2o,   #train on downsampled data\n",
    "              seed = 4313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2160fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get grid results, sorted by validation AUC\n",
    "gbm_gridperf2 = gbm_grid2.get_grid(sort_by = 'auc', decreasing = True)\n",
    "print(gbm_gridperf2)\n",
    "\n",
    "#grab top GBM model, chosen by validation AUC\n",
    "best_gbm2 = gbm_gridperf2.models[0]\n",
    "\n",
    "#AUC of cross-validation holdout predictions\n",
    "best_gbm2.auc(xval = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7d91c",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model as MOJO file\n",
    "model_path = best_gbm2.download_mojo(path = filepath, get_genmodel_jar = False)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34a001",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413dd4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MOJO file\n",
    "model_path = '/mapr/datalake/gbm_grid2_model.zip'\n",
    "imported_model = h2o.upload_mojo(model_path)\n",
    "print(\"model imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df7c81",
   "metadata": {},
   "source": [
    "## ====================\n",
    "## Auto ML\n",
    "## ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b434c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_model = H2OAutoML(max_models = 10, \n",
    "                     nfolds = 3,\n",
    "                     seed = 5421,\n",
    "                     include_alogs = ['xGBoost','GBM','DRF'])\n",
    "am_model.train(x = trn_cols,\n",
    "               y = y ,\n",
    "               training_frame = train_h2o)   #training on downsampled majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view leaderboard\n",
    "lb = am_model.leaderboard\n",
    "lb.head(rows = lb.nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best model using the metric\n",
    "best_model = aml_model.leader\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c23f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d01426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output to file\n",
    "tmp_df = pd.DataFrame()\n",
    "km_lst = list(best_model.params.keys())\n",
    "\n",
    "for i in km_lst:\n",
    "    df = pd.DataFrame.from_dict(best_model.params[i], orient='index', columns=[i])\n",
    "    tmp_df = pd.concat([tmp_df,df],axis=1)\n",
    "    \n",
    "tmp_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
