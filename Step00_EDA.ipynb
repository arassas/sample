{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee6c31f",
   "metadata": {},
   "source": [
    "# Step 00: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d64377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import IPython.core.display import display, HTML\n",
    "import scipy.stats\n",
    "\n",
    "#pyspark\n",
    "import pyspark.sql import SparkSession, DataFrame as SparkDataFrame\n",
    "import pyspark.sql.functions as F, isnan, when, count, col, to_date\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import QuantileDiscretizer, VectorAssembler\n",
    "import spark_df_profiling\n",
    "\n",
    "#scipy and statsmodels\n",
    "from scipt import stats\n",
    "from scipy.stats import friedmanchisquare, kruskal, wilcoxon, ks_2samp, chi2_contingency, chi2, norm\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.iolib.smpickle import load_pickle\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.compose import make_colum_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#light gbm\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#H20\n",
    "import h2o\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "\n",
    "#print versions\n",
    "print('sklearn:{}'.format(sklearn.__version__))\n",
    "print('pandas:{}'.format(pd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94379010",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e785a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_rate(path, sdf_input, label, sampling):\n",
    "    cols = sdf_inputs.columns\n",
    "    smpl_df = sdf_input.sample(withReplacement = False, fraction = sampling, seed=432).toPandas()\n",
    "    lst = []\n",
    "    \n",
    "    for col in cols:\n",
    "        values_dict = {}\n",
    "        values_dict['var'] = col\n",
    "        values_dict['tot_count'] = smpl_df[col].count()\n",
    "        values_dict['unique_val'] = smpl_df[col].unique()\n",
    "        values_dict['num_missing_rows'] = smpl_df[col].isnull().sum()\n",
    "        values_dict['missing_rate'] = (smpl_df[col].isnull().sum()/len(smpl_df[col]) )\n",
    "        values_dict['high_missing_rate'] = np.where( (smpl_df[col].isnull().sum()/len(smpl_df[col]))<= 0.95,0,1 )\n",
    "        \n",
    "        lst.append(values_dict)\n",
    "        \n",
    "    data_qa = pd.DataFrame(lst)\n",
    "    data_qa.sort_values(by = ['missing_rate'], ascending = False, inplace = True)\n",
    "    data_qa.to_csv(path + label + '_stats_missing.csv')\n",
    "    print('CSV file saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e2272b",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb29518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Hive table\n",
    "phys = spark.sql(\"\"\" select * from table \"\"\")\n",
    "phys = phys.filter(F.col('week_n') != 17)\n",
    "phys.createOrReplaceTempView('phys')\n",
    "\n",
    "# print stats\n",
    "print(\"Number of obs:\", phys.count())\n",
    "print(\"Number of cols:\", len(phys.columns))\n",
    "\n",
    "stats = phys.groupBy(['week_n']) \\ \n",
    "            .agg(F.sum(F.col('call_within30')).alias('tot positives'),\n",
    "                F.count('*').alias('tot rows'),\n",
    "                (F.sum(F.col('call_within30'))/F.count('*')).alias('target rate')).orderBy('week_n')\n",
    "\n",
    "df = stats.toPandas()\n",
    "\n",
    "#fill missing\n",
    "phys = phys.na.fill(value=0)\n",
    "\n",
    "#drop original target\n",
    "phys = phys.drop(*['call_within30'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c482b",
   "metadata": {},
   "source": [
    "## Weekly series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fdf831",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = ['clm_num','clm_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36623b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in feat:\n",
    "    stats = phys.groupBy(['week_n'])\\\n",
    "            .agg(F.avg(F.col(f))).orderBy('week_n')\n",
    "    stats.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1faae7",
   "metadata": {},
   "source": [
    "## find missing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_rate(path = perspath,\n",
    "                sdf_input = phys,\n",
    "                label = 'data',\n",
    "                sampling = 0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ce6c6",
   "metadata": {},
   "source": [
    "## numeric and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = phys.sample(withReplacement = False, fraction = 0.05, seed=432).toPandas()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57cebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hoosp_ind'] = df['hoosp_ind'].astype('object')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645fdf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# numeric\n",
    "# ---------------------------\n",
    "num_vars = list(df.select_dtypes(include = ['int32', 'int64', 'float32', 'float64']).columns)\n",
    "num_vars.remove('call_ind')\n",
    "print('There are', len(num_vars), 'numeric features in the list')\n",
    "print(num_vars)\n",
    "print()\n",
    "\n",
    "# -------------------\n",
    "# categorical\n",
    "# -------------------\n",
    "cat_vars = list(df.select_dtypes(include = ['object']).columns)\n",
    "\n",
    "# -----------------\n",
    "# full list\n",
    "# ------------------\n",
    "full_lst = list(df.columns)\n",
    "print(\"there are\", len(full_lst), \"total feat in the list\")\n",
    "\n",
    "# ---------------------\n",
    "# prepare all cols\n",
    "# ----------------------\n",
    "final_vars = num_vars + cat_vars\n",
    "print(\"there are\", len(final_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417aafa2",
   "metadata": {},
   "source": [
    "## A. Numeric EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac57fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = df.groupby(['week_n'])[num_vars].describe().reset_index()\n",
    "stats.to_csv(path + 'num_eda.csv')\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238bd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_vars:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.distplot(df[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e9dce",
   "metadata": {},
   "source": [
    "## B. Categorical EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ef188",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_vars:\n",
    "    print(\"variable\", col)\n",
    "    print(pd.crosstab(df[col],df['target'],margins=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_vars:\n",
    "    stats = phys.groupBy(col) \\\n",
    "                .agg(F.count('*').alias('tot rows'))\\\n",
    "                .orderBy(F.col('tot rows').desc())\n",
    "    stats.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e204d87",
   "metadata": {},
   "source": [
    "## C. Spark profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a29cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = spark_df_profiling.ProfileReport(phys)\n",
    "\n",
    "#export to HTML\n",
    "report.to_file(outputfile = path + 'data_sdf_profiling.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
