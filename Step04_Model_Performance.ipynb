{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b453e5be",
   "metadata": {},
   "source": [
    "# Step 04: Model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace942b8",
   "metadata": {},
   "source": [
    "## A. Lift, gains, Hosmer-Lemeshow, KS stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddaf346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gain_lift_ks(df, ntile, predictedCol, observed):\n",
    "    df = (df.withColumn('Decile',F.ntile(ntile).over(Window.orderBy(F.desc(predictedCol)))).groupBy(F.col('Decile'))\\\n",
    "         .agg(F.min(F.col(predictedCol)).alias('Min_Prob'),\n",
    "              F.max(F.col(predictedCol)).alias('Max_Prob'),\n",
    "              F.mean(F.col(observed)).alias('Response_rate'),\n",
    "              F.mean(F.col(predictedCol)).alias('Predicted_Response_rate'),\n",
    "              F.sum(F.col(observed)).alias('Events'),\n",
    "              F.count(F.lit(1)).alias('Count')\n",
    "             )\n",
    "         )\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    pdf['NonEvents'] = pdf['Count'] - pdf['Events']\n",
    "    #events / non-events rate\n",
    "    pdf['Event_rate'] = pdf[['Events']].apply(lambda x:x/float(x.sum()))\n",
    "    pdf['NonEvent_rate'] = pdf[['NonEvents']].apply(lambda x:x/float(x.sum()))\n",
    "    # cumulative event rate / non event rate\n",
    "    pdf['Cum_event_rate'] = pdf[['Event_rate']].cumsum()\n",
    "    pdf['Cum_Nonevent_rate'] = pdf[['NonEvent_rate']].cumsum()\n",
    "    # KS statistic\n",
    "    pdf['KS'] = round(100*(pdf['Cum_event_rate'] - pdf['Cum_nonevent_rate']),4)\n",
    "    # MAD\n",
    "    pdf['|Diff_actual_prediction|'] = abs(df['Response_rate'] - pdf['Predicted_response_rate'])\n",
    "    gainsLift = pdf\n",
    "    \n",
    "    #lift\n",
    "    responseRate = 100*sum(gainsLift['Events'])/sum(gainsLift['Count'])\n",
    "    gainsLift['Lift'] = 100*gainsLift[['Response Rate']]/responseRate\n",
    "    #Cumulative lift\n",
    "    gainsLift['Cum_response_rate'] = (gainsLift[\"Events\"].cumsum()/gainsLift['Count'].cumsum())\n",
    "    gainsLift['Cum_Lift'] = 100*gainsLift[['Cum_response_rate']]/responseRate\n",
    "    #data fraction\n",
    "    gainsLift['Data_fraction'] = 1.0/ntile\n",
    "    gainsLift['Cum_data_fraction'] = gainsLift[['Data_fraction']].cumsum()\n",
    "    \n",
    "    return gainsLift\n",
    "\n",
    "\n",
    "def plot_gainLift(df_gainsLift):\n",
    "    responseRate = 100*sum(df_gainsLift['Events'])/sum(df_gainsLift['Count'])\n",
    "    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(12,6))\n",
    "    ax1.plot(100*df_gainsLift['Cum_data_fraction'],100*df_gainsLift['Cum_event_rate'])\n",
    "    ax1.plot([0,100],[0,100],color='navy',lw=2,linestyle='--')\n",
    "    ax1.plot([0,responseRate,100],[0,100,100],color='navy',lw=2,linestyle='--')\n",
    "    ax1.set_xlabel('Percentile')\n",
    "    ax1.set_ylabel('Gains')\n",
    "    ax1.set_title('Gains curve')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    #plot lift\n",
    "    ax2.plot(100*df_gainsLift['Cum_Data_fraction'],df_gainsLift['Cum_lift'])\n",
    "    ax2.set_xlabel('Percentile')\n",
    "    ax2.set_ylabel('Lift')\n",
    "    ax2.set_title('Lift Curve')\n",
    "    ax2.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "def get_pct_dev(sdf, predictedCol, observed):\n",
    "    #pct_dev = (abs(sdf[predictedCol].sum() - sdf[observed].sum())/sdf[observed].sum())\n",
    "    \n",
    "    pct_dev_df = sdf.agg( F.abs(F.sum(F.col(predictedCol)) - F.sum(F.col(observed))).alias('abs_res'),\n",
    "                         F.sum(F.col(observed)).alias('sum_actual'),\n",
    "                         (F.abs(F.sum(F.col(predictedCol)) - F.sum(F.col(observed)))).alias('pct_dev')).toPandas()\n",
    "    return pct_dev_df.iloc[0,2]\n",
    "                               \n",
    "\n",
    "\n",
    "# Hosmer-Lemeshow   \n",
    "def HL_table_sdf(sdf,ntile,observed,predictedCol,sig):\n",
    "    sdf = (sdf.withColumn('Decile', F.ntile(ntile),over(Window.orderBy(F.desc(predictedCol)))).groupBy(F.col('Decile'))\\\n",
    "            .agg(F.min(F.col(predictedCol)).alias(\"Score_low\"),\n",
    "                 F.max(F.col(predictedCol)).alias(\"Score_high\"),\n",
    "                 F.count(F.lit(1)).alias(\"Total Obs\"),\n",
    "                 F.mean(F.col(observed)).alias(\"Act_Events_Pct\"),\n",
    "                 F.mean(F.col(predictedCol)).alias(\"Pred_Events_Pct\"),\n",
    "                 F.sum(F.col(observed)).alias(\"Act_Events\")\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    pdf = sdf.toPandas()\n",
    "    pdf['Exp_Events'] = (pdf['Total_Obs']*pdf['Pred_Events_Pct']).round()\n",
    "    pdf['Act_Non_Events'] = pdf['Total_Obs'] - pdf['Act_events']\n",
    "    pdf['Exp_Non_Events'] = (pdf['Total_Obs']*(1-pdf['Pred_Events_Pct'])).round()\n",
    "    pdf['HLstat_Events'] = np.square(pdf['Act_Events'] - pdf['Exp_Events'])/pdf['Exp_Events']\n",
    "    pdf['HLstat_NonEvents'] = np.square(pdf['Act_Non_Events'] - pdf['Exp_Non_Events'])/pdf['Exp_Non_Events']\n",
    "    pdf['HL_stat'] = pdf['HLStat_Events'] + pdf['HLStat_NonEvents']\n",
    "    \n",
    "    #check HL test significant\n",
    "    HL_stat = pdf['HL_stat'].sum()\n",
    "    d_f = pdf['Decile'].max()-2\n",
    "    p_value = 1-chi2.cdf(HL_stat,d_f)\n",
    "    \n",
    "    #signficant level\n",
    "    print(\"Hosmer-Lemeshow test statistics is {} and p-values is {} at degree of freedom {}\".format(round(HL_stat,2),\n",
    "                                                                                                   p_value,\n",
    "                                                                                                   d_f))\n",
    "    if p_value < sig:\n",
    "        print(\"HL test is significant at level {}, which indicates that the model is not a good fit\".format(sig))\n",
    "    else:\n",
    "        print(\"HL test is not significant at level {}, which indicates that the model is a good fit\".format(sig))\n",
    "        \n",
    "    #generate tables\n",
    "    df_out = pdf.to_html(formatters = {\n",
    "                        'Decile':'{:,.0f}'.format,\n",
    "                        'Total_Obs':'{:,.0f}'.format,\n",
    "                        'Act_events':'{:,.0f}'.format,\n",
    "                        'Act_Non_events':'{:,.0f}'.format,\n",
    "                        'Exp_events':'{:,.0f}'.format,\n",
    "                        'Exp_Non_events':'{:,.0f}'.format,\n",
    "                        'Act_Events_Pct':'{:,.2%}'.format,\n",
    "                        'Pred_Events_Pct':'{:,.2%}'.format})\n",
    "    display(HTML(df_out))\n",
    "    \n",
    "    \n",
    "\n",
    "# pozzolo formula\n",
    "def prob_adj_pozzolo(df, beta, p_col):\n",
    "    p_s = df[p_col]\n",
    "    p_adj = (p_s * beta) / (p_s * beta - p_s + 1)\n",
    "    df['P_adj'] = p_adj\n",
    "    return df\n",
    "\n",
    "\n",
    "# runs HL, deciles\n",
    "def generate_gains_lift_HL(sdf, path, name, target, orig_distr):\n",
    "    #calibrate both in-time/out-time validation since they reflect true distribition \n",
    "    #because training was downsampled, thus leading to higher probability than true distr\n",
    "    if orig_distr:\n",
    "        predictedCol = 'p_adj'\n",
    "    else:\n",
    "        predictedCol = 'p1'\n",
    "        \n",
    "    #Hosmer-Lemeshow\n",
    "    HL_df = HL_table_sdf(sdf = sdf,\n",
    "                        ntile = 10,\n",
    "                        observed = target,\n",
    "                        predictedCol = predictedCol,\n",
    "                        sig = 0.05)\n",
    "    \n",
    "    print(\"******************** KS *********************\")\n",
    "    gain_lift = get_gain_lift_ks(sdf, 10, predictedCol, target)\n",
    "    pct_dev = get_pct_dev(sdf, predictedCol, target)\n",
    "    print(\"Difference between predicted and actuals is {:0.7f}\".format(pct_dev))\n",
    "    \n",
    "    plot_gainLift(gain_lift)\n",
    "    gain_lift.to_csv(path + name + '_KSchart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270684b",
   "metadata": {},
   "source": [
    "## Decile chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decile_charts_train(path, scrn_data, name, clf_name):\n",
    "    \n",
    "    #read spark dataframe using raw prob \"p1\" because we use down-sampled data\n",
    "    df = (scrn_data.withColumn('Decile',F.ntile(10).over(Window.orderBy(F.desc('p1')))).groupBy(F.col('Decile'))\\\n",
    "        .agg(F.min(F.col('p1')).alias('Min_Prob'),  #praw\n",
    "            F.max(F.col('p1')).alias('Max_Prob'),\n",
    "            F.mean(F.col('target')).alias('Actual'),\n",
    "            F.mean(F.col('p1')).alias('Raw_Prob'),\n",
    "            F.sum(F.col('target')).alias('Events'),\n",
    "            F.count(F.lit(1)).alias('Min_Prob'))\n",
    "        )\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    #place max probability into dataframe\n",
    "    raw_pred = pd.DataFrame(pdf['max_probability'])\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # save cutoffs from training set\n",
    "    # ---------------------------------------\n",
    "    interval_lst = raw_pred['max_prob'].values.tolist()\n",
    "    interval_lst.sort(reverse=False)\n",
    "    interval_lst.insert(0, -float('inf'))\n",
    "    interval_lst.pop()\n",
    "    interval_lst.append(float('inf'))\n",
    "    \n",
    "    #pickle file\n",
    "    filename = (path + clf_name + '_train_decile_cutoffs')\n",
    "    pickle.dump(interval_lst, open(filename,'wb'))\n",
    "    print('Cutoffs saved to pickle')\n",
    "    \n",
    "    # ===================================\n",
    "    # print final table \n",
    "    # ===================================\n",
    "    pdf.to_csv(path + name + '_decile_tbl.csv')\n",
    "    print('table hg been saved to CSV')\n",
    "    \n",
    "    # ===================================\n",
    "    # plot figure\n",
    "    # ===================================\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(np.array(pdf['Decile']),np.array(pdf['Actual']),marker='.',marketsize=10,label='Actual Response Rate')\n",
    "    plt.plot(np.array(pdf['Decile']),np.array(pdf['RawProb']),marker='.',marketsize=10,label='Raw Probability')\n",
    "    plt.xlabel('Decile')\n",
    "    plt.ylabel('Mean Probability')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Plot of mean probabilities')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def decile_charts_val(path, scrn_data, name, clf_name):\n",
    "    decile_map = {0:10,\n",
    "                     1:9,\n",
    "                     2:8,\n",
    "                     3:7,\n",
    "                     4:6,\n",
    "                     5:5,\n",
    "                     6:4,\n",
    "                     7:3,\n",
    "                     8:2,\n",
    "                     9:1}\n",
    "        \n",
    "    # =============================================\n",
    "    # apply decile cutoffs on validation sets\n",
    "    # =============================================\n",
    "    filename = (path + clf_name + '_train_decile_cutoffs')\n",
    "    cutoffs = pickle.load(open(filename,'rb'))\n",
    "    print('Cutoffs has been loaded')\n",
    "    \n",
    "    #read spark dataframe using raw \"p1\"\n",
    "    bucketizer = Bucketizer(splits = cutoffs, inputCol='p1', outputCol = 'buckets')  #p_adj is adjusted, praw \n",
    "    scrn_data_n = bucketizer.setHandleInvalid('keep').transform(scrn_data)\n",
    "    \n",
    "    df = scrn_data_n.groupBy('buckets')\\\n",
    "                    .agg(F.mean(F.col('target')).alias('actual'),\n",
    "                         F.sum(F.col('target')).alias('tot events'),\n",
    "                         F.mean(F.col('p_adj')).alias('RawProb'),   #reporting using calibrated P\n",
    "                         F.min(F.col('p_adj')).alias('min_predict_prob'),\n",
    "                         F.max(F.col('p_adj')).alias('max_predict_prob'),\n",
    "                         F.count(F.lit(1)).alias('Count'))\n",
    "    pdf = df.toPandas()\n",
    "    pdf = pdf.sort_values(by = 'buckets', ascending = False)   #bucket #9 represents first decile\n",
    "    pdf['buckets'] = pdf['buckets'].map(decile_map)\n",
    "    \n",
    "    \n",
    "    # =============================================\n",
    "    # print final table\n",
    "    # =============================================\n",
    "    pdf.to_csv(path + name + '_decile_tbl.csv')\n",
    "    print('table has been saved to CSV')\n",
    "    \n",
    "    \n",
    "    # =============================================\n",
    "    # plot figure\n",
    "    # =============================================\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(np.array(pdf['buckets']),np.array(pdf['Actual']),marker='.',marketsize=10,label='Actual Response Rate')\n",
    "    plt.plot(np.array(pdf['buckets']),np.array(pdf['RawProb']),marker='.',marketsize=10,label='Raw Probability')\n",
    "    plt.xlabel('Decile')\n",
    "    plt.ylabel('Mean Probability')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Plot of mean probabilities')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf52682",
   "metadata": {},
   "source": [
    "## General performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs > threshold).astype('int')\n",
    "\n",
    "def calc_model_perf_train(name, X, smpl, pcol):\n",
    "    #declare new dict to store results\n",
    "    model_perf = {}\n",
    "    model_perf['smpl'] = smpl\n",
    "    model_perf['model'] = name\n",
    "    model_perf['brier_score'] = brier_score_loss(X['target'],X[pcol])\n",
    "    model_perf['log_loss'] = log_loss(X['target'],X[pcol])\n",
    "    model_perf['ROC'] = roc_auc_score(X['target'],X[pcol])\n",
    "    \n",
    "    #calculate precision and recall values\n",
    "    precision, recall, thresholds = precision_recall_curve(X['target'].values, x[pcol].values)\n",
    "    model_perf=['AUC using precision-recall values'] = auc(recall,precision)\n",
    "    \n",
    "    # ===========================================\n",
    "    # Option 0: using default 50% threshold \n",
    "    # ===========================================\n",
    "    model_perf['tot_pred_class_default'] = X['pclass'].sum()\n",
    "    model_perf['accuracy_default'] = accuracy_score(X['target'],X['pclass'])\n",
    "    model_perf['precision_default'] = precision_score(X['target'],X['pclass'])\n",
    "    model_perf['recall_default'] = recall_score(X['target'],X['pclass'])\n",
    "    model_perf['f1_score_default'] = f1_score(X['target'],X['pclass'])\n",
    "    \n",
    "    tbl = pd.crosstab(X['pclass'],X['target'],margins=True)\n",
    "    model_perf['TPR_class_default'] = (tbl.iloc[1,1] / tbl.iloc[2,1])\n",
    "    model_perf['FPR_class_default'] = (tbl.iloc[1,0] / tbl.iloc[2,0])\n",
    "    model_perf['FNR_class_default'] = (tbl.iloc[0,1] / tbl.iloc[2,1])\n",
    "    model_perf['TNR_class_default'] = (tbl.iloc[0,0] / tbl.iloc[2,0])\n",
    "    model_perf['PPV_class_default'] = (tbl.iloc[1,1] / tbl.iloc[1,2])\n",
    "    \n",
    "    \n",
    "    # ========================================================\n",
    "    # Option 1: using ROC curve to find optimal threshold\n",
    "    # ========================================================\n",
    "    fpr, tpr, thresholds = roc_curve(X['target'].values, X[pcol].values)\n",
    "    \n",
    "    #get best threshold\n",
    "    J = tpr - fpr\n",
    "    ix = np.argmax(J)\n",
    "    best_thresh = thresholds[ix]\n",
    "    print(\"Best threshold using ROC curve=%f\" %(best_thresh))\n",
    "    \n",
    "    #optimize class using best thresholds\n",
    "    X['class_roc'] = np.where(X[pcol] > best_thresh,1,0)\n",
    "    \n",
    "    model_perf['tot_pred_class_roc'] = X['class_roc'].sum()\n",
    "    model_perf['accuracy_class_roc'] = accuracy_score(X['target'],X['class_roc'])\n",
    "    model_perf['precision_class_roc'] = precision_score(X['target'],X['class_roc'])\n",
    "    model_perf['recall_class_roc'] = recall_score(X['target'],X['class_roc'])\n",
    "    model_perf['f1_score_class_roc'] = f1_score(X['target'],X['class_roc'])\n",
    "    \n",
    "    tbl = pd.crosstab(X['class_roc'],X['target'],margins=True)\n",
    "    model_perf['TPR_class_roc'] = (tbl.iloc[1,1] / tbl.iloc[2,1])\n",
    "    model_perf['FPR_class_roc'] = (tbl.iloc[1,0] / tbl.iloc[2,0])\n",
    "    model_perf['FNR_class_roc'] = (tbl.iloc[0,1] / tbl.iloc[2,1])\n",
    "    model_perf['TNR_class_roc'] = (tbl.iloc[0,0] / tbl.iloc[2,0])\n",
    "    model_perf['PPV_class_roc'] = (tbl.iloc[1,1] / tbl.iloc[1,2])\n",
    "    \n",
    "    \n",
    "    # =====================================================================\n",
    "    # Option 2: using precision-recall curve to find optimal threshold\n",
    "    # =====================================================================\n",
    "    #calculate pr curve\n",
    "    precision, recall, thresholds = precision_recall_curve(X['target'].values, X[pcol].values)\n",
    "    fscore = (2 * precision * recall)/(precision + recall)\n",
    "    \n",
    "    #located index of largest f-score\n",
    "    ix = np.argmax(fscore)\n",
    "    print(\"best threshold using precision-recall=%f, F-score=%0.3f\" %(thresholds[ix],fscore[ix]))\n",
    "    \n",
    "    #optimize class using best threshold\n",
    "    X['class_pr'] = np.where(X[pcol] > thresholds[ix],1,0)\n",
    "    \n",
    "    model_perf['tot_pred_class_pr'] = X['class_pr'].sum()\n",
    "    model_perf['accuracy_class_pr'] = accuracy_score(X['target'],X['class_pr'])\n",
    "    model_perf['precision_class_pr'] = precision_score(X['target'],X['class_pr'])\n",
    "    model_perf['recall_class_pr'] = recall_score(X['target'],X['class_pr'])\n",
    "    model_perf['f1_score_class_pr'] = f1_score(X['target'],X['class_pr'])\n",
    "    \n",
    "    tbl = pd.crosstab(X['class_pr'],X['target'],margins=True)\n",
    "    model_perf['TPR_class_pr'] = (tbl.iloc[1,1] / tbl.iloc[2,1])\n",
    "    model_perf['FPR_class_pr'] = (tbl.iloc[1,0] / tbl.iloc[2,0])\n",
    "    model_perf['FNR_class_pr'] = (tbl.iloc[0,1] / tbl.iloc[2,1])\n",
    "    model_perf['TNR_class_pr'] = (tbl.iloc[0,0] / tbl.iloc[2,0])\n",
    "    model_perf['PPV_class_pr'] = (tbl.iloc[1,1] / tbl.iloc[1,2])\n",
    "    \n",
    "    \n",
    "    # ===========================================================================\n",
    "    # Option 3: using precision-recall curve to find optimal threshold (tuning)\n",
    "    # ===========================================================================\n",
    "    #define thresholds\n",
    "    thresholds2 = np.arange(0,1,0.001)\n",
    "    \n",
    "    #evaluate best threshold\n",
    "    scores = [f1_score(X['target'].values, to_labels(X[pcol].values,t)) for t in thresholds2]\n",
    "    \n",
    "    #get best tresholds\n",
    "    ix = np.argmax(scores)\n",
    "    print(\"Best threshold using precision-recall=%f, F-score=%0.3f\" % (threhsolds2[ix],scores[ix]))\n",
    "    \n",
    "    #optimize class using best threshold\n",
    "    X['class_pr_tuning'] = np.where(X[pcol] > thresholds2[ix],1,0)\n",
    "    \n",
    "    model_perf['tot_pred_class_pr_tuning'] = X['class_pr_tuning'].sum()\n",
    "    model_perf['accuracy_class_pr_tuning'] = accuracy_score(X['target'],X['class_pr_tuning'])\n",
    "    model_perf['precision_class_pr_tuning'] = precision_score(X['target'],X['class_pr_tuning'])\n",
    "    model_perf['recall_class_pr_tuning'] = recall_score(X['target'],X['class_pr_tuning'])\n",
    "    model_perf['f1_score_class_pr_tuning'] = f1_score(X['target'],X['class_pr_tuning'])\n",
    "    \n",
    "    tbl = pd.crosstab(X['class_pr_tuning'],X['target'],margins=True)\n",
    "    model_perf['TPR_class_pr_tuning'] = (tbl.iloc[1,1] / tbl.iloc[2,1])\n",
    "    model_perf['FPR_class_pr_tuning'] = (tbl.iloc[1,0] / tbl.iloc[2,0])\n",
    "    model_perf['FNR_class_pr_tuning'] = (tbl.iloc[0,1] / tbl.iloc[2,1])\n",
    "    model_perf['TNR_class_pr_tuning'] = (tbl.iloc[0,0] / tbl.iloc[2,0])\n",
    "    model_perf['PPV_class_pr_tuning'] = (tbl.iloc[1,1] / tbl.iloc[1,2])\n",
    "    \n",
    "    #prepare final dataframe\n",
    "    return pd.DataFrame.from_dict(model_perf, orient = 'index', columns=['metrics'])\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# Using this on validation - threshold from training\n",
    "# ===========================================================================\n",
    "def calc_model_perf_val(name, X, smpl, roc_thresh, pr_thresh, pr_tuning_thresh, pcol):\n",
    "    \n",
    "    #declare new dict to store results\n",
    "    model_perf = {}\n",
    "    model_perf['smpl'] = smpl\n",
    "    model_perf['model'] = name\n",
    "    model_perf['brier_score'] = brier_score_loss(X['target'],X[pcol])\n",
    "    model_perf['log_loss'] = log_loss(X['target'],X[pcol])\n",
    "    model_perf['ROC'] = roc_auc_score(X['target'],X[pcol])\n",
    "    \n",
    "    #calculate precision and recall values\n",
    "    precision, recall, thresholds = precision_recall_curve(X['target'].values, x[pcol].values)\n",
    "    model_perf=['AUC using precision-recall values'] = auc(recall,precision)\n",
    "    \n",
    "    # ===========================================\n",
    "    # Option 0: using default 50% threshold \n",
    "    # ===========================================\n",
    "    model_perf['tot_pred_class_default'] = X['pclass'].sum()\n",
    "    model_perf['accuracy_default'] = accuracy_score(X['target'],X['pclass'])\n",
    "    model_perf['precision_default'] = precision_score(X['target'],X['pclass'])\n",
    "    model_perf['recall_default'] = recall_score(X['target'],X['pclass'])\n",
    "    model_perf['f1_score_default'] = f1_score(X['target'],X['pclass'])\n",
    "    \n",
    "    tbl = pd.crosstab(X['pclass'],X['target'],margins=True)\n",
    "    model_perf['TPR_class_default'] = (tbl.iloc[1,1] / tbl.iloc[2,1])\n",
    "    model_perf['FPR_class_default'] = (tbl.iloc[1,0] / tbl.iloc[2,0])\n",
    "    model_perf['FNR_class_default'] = (tbl.iloc[0,1] / tbl.iloc[2,1])\n",
    "    model_perf['TNR_class_default'] = (tbl.iloc[0,0] / tbl.iloc[2,0])\n",
    "    model_perf['PPV_class_default'] = (tbl.iloc[1,1] / tbl.iloc[1,2])\n",
    "    \n",
    "    \n",
    "    # ========================================================\n",
    "    # Option 1: using ROC curve to find optimal threshold\n",
    "    # ========================================================\n",
    "    #optimize class using best thresholds\n",
    "    X['class_roc'] = np.where(X[pcol] > roc_thresh,1,0)\n",
    "    \n",
    "    model_perf['tot_pred_class_roc'] = X['class_roc'].sum()\n",
    "    model_perf['accuracy_class_roc'] = accuracy_score(X['target'],X['class_roc'])\n",
    "    model_perf['precision_class_roc'] = precision_score(X['target'],X['class_roc'])\n",
    "    model_perf['recall_class_roc'] = recall_score(X['target'],X['class_roc'])\n",
    "    model_perf['f1_score_class_roc'] = f1_score(X['target'],X['class_roc'])\n",
    "    \n",
    "    tbl = pd.crosstab(X['class_roc'],X['target'],margins=True)\n",
    "    model_perf['TPR_class_roc'] = (tbl.iloc[1,1] / tbl.iloc[2,1])\n",
    "    model_perf['FPR_class_roc'] = (tbl.iloc[1,0] / tbl.iloc[2,0])\n",
    "    model_perf['FNR_class_roc'] = (tbl.iloc[0,1] / tbl.iloc[2,1])\n",
    "    model_perf['TNR_class_roc'] = (tbl.iloc[0,0] / tbl.iloc[2,0])\n",
    "    model_perf['PPV_class_roc'] = (tbl.iloc[1,1] / tbl.iloc[1,2])\n",
    "    \n",
    "    \n",
    "    # =====================================================================\n",
    "    # Option 2: using precision-recall curve to find optimal threshold\n",
    "    # =====================================================================\n",
    "    #optimize class using best threshold\n",
    "    X['class_pr'] = np.where(X[pcol] > pr_thresh,1,0)\n",
    "    \n",
    "    model_perf['tot_pred_class_pr'] = X['class_pr'].sum()\n",
    "    model_perf['accuracy_class_pr'] = accuracy_score(X['target'],X['class_pr'])\n",
    "    model_perf['precision_class_pr'] = precision_score(X['target'],X['class_pr'])\n",
    "    model_perf['recall_class_pr'] = recall_score(X['target'],X['class_pr'])\n",
    "    model_perf['f1_score_class_pr'] = f1_score(X['target'],X['class_pr'])\n",
    "    \n",
    "    tbl = pd.crosstab(X['class_pr'],X['target'],margins=True)\n",
    "    model_perf['TPR_class_pr'] = (tbl.iloc[1,1] / tbl.iloc[2,1])\n",
    "    model_perf['FPR_class_pr'] = (tbl.iloc[1,0] / tbl.iloc[2,0])\n",
    "    model_perf['FNR_class_pr'] = (tbl.iloc[0,1] / tbl.iloc[2,1])\n",
    "    model_perf['TNR_class_pr'] = (tbl.iloc[0,0] / tbl.iloc[2,0])\n",
    "    model_perf['PPV_class_pr'] = (tbl.iloc[1,1] / tbl.iloc[1,2])\n",
    "    \n",
    "    # ===========================================================================\n",
    "    # Option 3: using precision-recall curve to find optimal threshold (tuning)\n",
    "    # ===========================================================================\n",
    "    #optimize class using best threshold\n",
    "    X['class_pr_tuning'] = np.where(X[pcol] > pr_tuning_thresh,1,0)\n",
    "    \n",
    "    model_perf['tot_pred_class_pr_tuning'] = X['class_pr_tuning'].sum()\n",
    "    model_perf['accuracy_class_pr_tuning'] = accuracy_score(X['target'],X['class_pr_tuning'])\n",
    "    model_perf['precision_class_pr_tuning'] = precision_score(X['target'],X['class_pr_tuning'])\n",
    "    model_perf['recall_class_pr_tuning'] = recall_score(X['target'],X['class_pr_tuning'])\n",
    "    model_perf['f1_score_class_pr_tuning'] = f1_score(X['target'],X['class_pr_tuning'])\n",
    "    \n",
    "    tbl = pd.crosstab(X['class_pr_tuning'],X['target'],margins=True)\n",
    "    model_perf['TPR_class_pr_tuning'] = (tbl.iloc[1,1] / tbl.iloc[2,1])\n",
    "    model_perf['FPR_class_pr_tuning'] = (tbl.iloc[1,0] / tbl.iloc[2,0])\n",
    "    model_perf['FNR_class_pr_tuning'] = (tbl.iloc[0,1] / tbl.iloc[2,1])\n",
    "    model_perf['TNR_class_pr_tuning'] = (tbl.iloc[0,0] / tbl.iloc[2,0])\n",
    "    model_perf['PPV_class_pr_tuning'] = (tbl.iloc[1,1] / tbl.iloc[1,2])\n",
    "    \n",
    "    #prepare final dataframe\n",
    "    return pd.DataFrame.from_dict(model_perf, orient = 'index', columns=['metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2537ca2a",
   "metadata": {},
   "source": [
    "## Launch H2o cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pysparkling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbad8ba",
   "metadata": {},
   "source": [
    "## Load models from H2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704dfa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# GBM\n",
    "# ---------------------------\n",
    "name = 'gbm_sel'\n",
    "n_set = 'gbm'\n",
    "\n",
    "model_path = '/mapr/datalake/gbm_grid2_model.zip'\n",
    "imported_model = h2o.upload_mojo(model_path)\n",
    "print(\"model imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd043f",
   "metadata": {},
   "source": [
    "## Read scoring files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba69300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# training downsampled\n",
    "# ------------------------------------\n",
    "trn_sdf_orig = spark.read.parquet(path + 'train_feat_se_r2.parquet')\n",
    "print(\"number of obs:\", trn_sdf_orig.count())\n",
    "print(\"number of cols:\", len(trn_sdf_orig.columns))\n",
    "\n",
    "# ------------------------------------\n",
    "# testing / val\n",
    "# ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa637d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of primary key (dates, claim, prov_id)\n",
    "pk_lst = ['claim_nbr']\n",
    "\n",
    "# list of numeric var that must be treated as factor in H2o\n",
    "binary_lst = ['hosp_ind']\n",
    "\n",
    "print(\"*****************************\")\n",
    "print(\"Before removing PK\")\n",
    "print(\"*****************************\")\n",
    "# ---------------------------\n",
    "# numeric\n",
    "# ---------------------------\n",
    "num_cols = []\n",
    "for item in trn_sdf.dtypes:\n",
    "    if item[1].startswith('int') | item[1].startswith('double') | item[1].startswith('bigint'):\n",
    "        num_cols.append(item[0])\n",
    "        \n",
    "print(\"Numeric list:\", num_cols)\n",
    "print(\"there are\", len(num_cols), \"numeric attributes, including target\")\n",
    "print()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# categorical\n",
    "# ---------------------------\n",
    "cat_cols = []\n",
    "for item in trn_sdf.dtypes:\n",
    "    if item[1].startswith('string'):\n",
    "        cat_cols.append(item[0])\n",
    "        \n",
    "print(\"Categorical list:\", cat_cols)\n",
    "print(\"there are\", len(cat_cols), \"categorical attributes, including target\")\n",
    "print() \n",
    "\n",
    "\n",
    "\n",
    "#remove PK list\n",
    "print(\"*****************************\")\n",
    "print(\"After removing PK\")\n",
    "print(\"*****************************\")\n",
    "num_lst_n = []\n",
    "for i in num_cols:\n",
    "    if i not in pk_lst and i not in binary_lst:\n",
    "        num_lst_n.append(i)\n",
    "        \n",
    "print(\"numeric list:\", num_lst_n)\n",
    "print('there are', len(num_lst_n), 'numeric attributes, including target')\n",
    "\n",
    "\n",
    "cat_lst_n = []\n",
    "for i in cat_cols:\n",
    "    if i not in pk_lst:\n",
    "        cat_lst_n.append(i)\n",
    "        \n",
    "# add binary variables into categorical\n",
    "cat_lst_n.extend(binary_lst)\n",
    "print(\"Categorical list:\", cat_lst_n)\n",
    "print(\"there are\", len(cat_lst_n), \"categorical attributes, including target\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"*****************************\")\n",
    "print(\"Final list\")\n",
    "print(\"*****************************\")\n",
    "# merge into one list\n",
    "lst = num_lst_n + cat_lst_n\n",
    "print(\"Final list:\", lst)\n",
    "print(\"there are\", len(lst), \"categorical attributes, including target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf78bbb",
   "metadata": {},
   "source": [
    "## Transform into h2o frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35caf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'target'\n",
    "\n",
    "# --------------------\n",
    "# in-time training\n",
    "# --------------------\n",
    "print(\"preparing training\")\n",
    "#trn_sdf_n = trn_sdf.select(*lst)  #only required cols\n",
    "train_h2o = hc.asH2OFrame(trn_sdf_orig)\n",
    "\n",
    "#prepare final list of attribres - converting categorical as factor\n",
    "for i in cat_lst_n:\n",
    "    train_h2o[i] = train_h2o[i].asfactor()\n",
    "    \n",
    "# validation and OOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_h2o.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d65258",
   "metadata": {},
   "source": [
    "## Score datasets and calibrate raw probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.0005432\n",
    "\n",
    "# -------------------------------------\n",
    "# training\n",
    "# -------------------------------------\n",
    "print(\"score training\")\n",
    "pred_trn = imported_model.predict(train_h2o)\n",
    "pred_trn = pred_trn.drop(['p0'])\n",
    "# concatenate to original dataset\n",
    "trn_h2o = train_h2o.cbind(pred_trn)\n",
    "trn_sdf_n = hc.asSparkFrame(trn_h2o)\n",
    "trn_sdf_n = trn_sdf_n.withColumnRenamed('call','target')\n",
    "trn_sdf_n = trn_sdf_n.withColumnRenamed('predict','pclass')\n",
    "trn_sdf_n = trn_sdf_n.withColumn('target', F.col('target').cast('int'))\n",
    "trn_sdf_n = trn_sdf_n.withColumn('p_adj', ( F.col('p1')*F.lit(beta) / (F.col('p1')*F.lit(beta) - F.col('p1') + F.lit(1)) ))\n",
    "\n",
    "# ------------------------\n",
    "# stats\n",
    "# ------------------------\n",
    "print(\"************** training ****************\")\n",
    "trn_sdf_n.agg(F.sum(F.col('p1')).alias('total predicted target: raw prob'),\n",
    "              F.sum(F.col(['p_adj'])).alias('total predicted target: adj prob'),\n",
    "              (F.sum(F.col('target'))/F.countDistrinct(F.col('claim_nbr'))).alias('call rate'),\n",
    "              F.sum(F.col('target')).alias('total actual target'),\n",
    "              F.countDistinct(F.col('claim_nbr')).alias('total claims')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1188398",
   "metadata": {},
   "source": [
    "## dataframes for model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223bc078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "trn_df = trn_sdf_n.select('target','pclass','p1').toPandas()\n",
    "trn_df['pclass'] = trn_df['pclass'].astype('int')\n",
    "\n",
    "print(\"training:\", trn_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a434cd",
   "metadata": {},
   "source": [
    "## model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdeb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# in-time training\n",
    "# ----------------------------------\n",
    "model_perf_train = {}\n",
    "model_train = imported_model.model_performance(train_h2o)\n",
    "model_perf_train['model'] = n_set\n",
    "model_perf_train['set'] = 'in-time training'\n",
    "model_perf_train['KS'] = imported_model.kolmogorov_smirnov()\n",
    "model_perf_train['AUC'] = model_train.auc()\n",
    "model_perf_train['AUCPR'] = model_train.aucpr()\n",
    "model_perf_train['Gini'] = model_train.gini()\n",
    "model_perf_train['LogLoss'] = model_train.logloss()\n",
    "model_perf_train['Precision'] = model_train.precision()\n",
    "model_perf_train['Recall'] = model_train.recall()\n",
    "model_perf_train['F1'] = model_train.F1()\n",
    "model_perf_train['Accuracy'] = model_train.accuracy()\n",
    "model_perf_train['best_thresh_F1'] = model_train.find_threshold_by_max_metric(\"f1\")\n",
    "\n",
    "#prepare table of metrics\n",
    "perf_df = pd.concat([pd.DataFrame(model_perf_train),\n",
    "                     pd.DataFrame(model_perf_test),\n",
    "                     pd.DataFrame(model_perf_oot)],axis=0,sort=False)\n",
    "perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259fc2a",
   "metadata": {},
   "source": [
    "### =========================================================\n",
    "### using scikit learn \n",
    "### ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7bc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score uncalibrated model\n",
    "saved_clf = load_model(path = path, name = 'xgboost')\n",
    "pred_trn = pd.concat([pd.DataFrame(saved_clf.predict_prob(train[lst].values)[:,1],columns=['praw']),\n",
    "                    pd.DataFrame(saved_clf.predict(train[lst].values),columns=['pclass'])],axis=1)\n",
    "print(\"training predictions complete\")\n",
    "\n",
    "\n",
    "#score calibrated model\n",
    "saved_clf_cal = load_model(path = path, name = 'xgboost_platt')\n",
    "cal_pred_trn = pd.DataFrame(saved_clf_cal.predict(pred_trn['praw']),columns=['pcal'])\n",
    "print(\"training predictions complete\")\n",
    "\n",
    "#merge into one data\n",
    "train_df = pd.concat([train,pred_trn,cal_pred_trn],axis=1)\n",
    "\n",
    "#select required fields\n",
    "trn_df = trn_sdf_n.select(['target','praw','pcal','pclass']).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee240d",
   "metadata": {},
   "source": [
    "### Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a48759",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sdf_n.agg(F.sum(F.col('praw')).alias('total predicted target: raw prob'),\n",
    "             F.sum(F.col('pcal')).alias('total predicted target: adj prob'),\n",
    "              (F.sum(F.col('target'))/F.countDistinct(F.col('id'))).alias('target rate'),\n",
    "              F.sum(F.col('target')).alias('total actual target'),\n",
    "              F.countDistinct(F.col('id')).alias('total mbrs')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b3da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bea7cc0b",
   "metadata": {},
   "source": [
    "# A. Lift, gains, HL stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61412b3e",
   "metadata": {},
   "source": [
    "## Model 1 GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf83d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*********** in-time training ***************\")\n",
    "generate_gains_lift_HL(sdf = trn_sdf_n,\n",
    "                      path = path,\n",
    "                      name = n_set + '_train',\n",
    "                      target = 'target',\n",
    "                      orig_distr = False)   #downsampled\n",
    "\n",
    "\n",
    "print(\"*********** in-time testing ***************\")\n",
    "generate_gains_lift_HL(sdf = tst_sdf_n,\n",
    "                      path = path,\n",
    "                      name = n_set + '_test',\n",
    "                      target = 'target',\n",
    "                      orig_distr = True)   #true distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lift = imported_model.gains_lift().as_data_frame()\n",
    "trn_lift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a8d57",
   "metadata": {},
   "source": [
    "# B. Decile charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a298c2",
   "metadata": {},
   "source": [
    "## Model 1: GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa038ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*********** in-time training ***************\")\n",
    "decile_charts_train(path = path,\n",
    "                   scrn_data = trn_sdf_n,\n",
    "                   name = n_set + '_train',\n",
    "                   clf_name = n_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0211b3c3",
   "metadata": {},
   "source": [
    "# C. Explainable AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e0699",
   "metadata": {},
   "source": [
    "## A. Feature importance on training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaeb569",
   "metadata": {},
   "source": [
    "### Model 01: GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269455b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lst_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42966e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "exm = imported_model.explain(train_h2o,\n",
    "                            columns = num_lst_n[:8],\n",
    "                            exclude_explanations = ['model_correlaiton_heatmap','shap_summary','ice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imported_model.varimp_plot()\n",
    "varimp = imported_moedl.varimp(use_pandas = True)\n",
    "varimp.to_csv(filepath + n_set + 'featImp.csv')\n",
    "varimp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1df6b9",
   "metadata": {},
   "source": [
    "## B. Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" *********************** In-Time Training *********************\")\n",
    "perf_df = calc_model_perf_train(name, trn_df, smpl = n_set + '_train', pcol = 'p1')\n",
    "df1 = perf_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288136ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_thresh = 0.18432\n",
    "pr_thresh = 0.342434\n",
    "pr_tuning = 0.3424\n",
    "\n",
    "print(\"********* IN TIME TESTING ***************\")\n",
    "perf_df = calc_model_perf_train(name, tst_df, smpl = n_set + '_test', \n",
    "                                roc_thresh = roc_thresh,\n",
    "                                pr_thresh = pr_thresh,\n",
    "                                pr_tuning = pr_tuning,\n",
    "                                pcol = 'p_adj')\n",
    "df2 = perf_df.T\n",
    "\n",
    "#print final table\n",
    "perf_final = pd.concat([df1,df2,df3],axis=0)\n",
    "perf_final.to_csv(filepath + n_set + '_modelPerf.csv')\n",
    "perf_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74295e65",
   "metadata": {},
   "source": [
    "## C. PDP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c5d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lst_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc42e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = []\n",
    "for col in num_lst_n:\n",
    "    col_lst.append(col)\n",
    "    imported_model.partial_plot(data = oot_h2o,\n",
    "                               cols = col_lst,\n",
    "                               server=True,\n",
    "                               plot=True,\n",
    "                               figsize=(6,4),\n",
    "                               nbins=20)\n",
    "    cols_lst.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_plot = imported_model.pd_plot(oot_h2o, 'tot_clm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_model.partial_plot(data = oot_h2o, cols = ['clm_lst6'], server = True, plot = True, figsize=(8,6),\n",
    "                                                   nbins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60557af7",
   "metadata": {},
   "source": [
    "## D. Feature permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H2o\n",
    "imported_model.permutation_importance_plot(oot_h2o)\n",
    "\n",
    "# scikitlearn\n",
    "from sklearn.inspection import permutation_importance\n",
    "train_result = permutation_importance(\n",
    "    saved_clf, X, y, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx = train_result.importances_mean.argsort()\n",
    "\n",
    "#prepare data for figure\n",
    "train_importances = pd.DataFrame(\n",
    "    train_result.importances[sorted_importances_idx].T,\n",
    "    columns = X.columns[sorted_importances_idx],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10,6]\n",
    "plt.rcParams['figure.autolayout'] = True\n",
    "\n",
    "for name, importances in zip(['training','in-time validation','out-time validation'],\n",
    "                            [train_importances, test_importances, oot_importances]):\n",
    "    ax = importances.plot.box(vert=False, whis=10)\n",
    "    ax.set_title(f\"Permutation Importances ({name} set)\")\n",
    "    ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "    ax.axvline(x=0, color = 'k', linestyle = '--')\n",
    "    ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807de375",
   "metadata": {},
   "source": [
    "## E. Find optimal cutoffs values\n",
    "https://medium.com/swlh/determining-a-cut-off-or-threshold-when-working-with-a-binary-dependent-target-variable-7c2342cf2a7c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef236f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code fragment to add to existing\n",
    "\n",
    "#true positive rate (1-FNR)\n",
    "tips['TPR'] = 1 - (1-tips['SENSITIVITY'])\n",
    "#true negative rate (1 - TPR)\n",
    "tips['TNR'] = 1 - (1-tips['SPECIFICITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009c37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = pd.crosstab(train_df['pclass'], train_df['target'], margins = True)\n",
    "print(\"TPR class pr\", (tbl.iloc[1,1]/tbl.iloc[2,1]))\n",
    "print(\"FPR class pr\", (tbl.iloc[1,0]/tbl.iloc[2,0]))\n",
    "print(\"FNR class pr\", (tbl.iloc[0,1]/tbl.iloc[2,1]))\n",
    "tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b6d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_df['pclass'], train_df['target']).apply(lambda r:r/r.sum(),axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
